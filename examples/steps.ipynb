{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf18fdd3",
   "metadata": {},
   "source": [
    "# Step Classes - Building Blocks of Pipelines\n",
    "\n",
    "This notebook demonstrates the core Step classes that form the building blocks of pipelines. Steps are reusable components that encapsulate data logic with clear contracts for inputs and outputs.\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "In this tutorial, you'll discover how to:\n",
    "\n",
    "1. **Create Basic Steps** - Define atomic units of work with input/output contracts\n",
    "2. **Handle Step Composition** - Combine steps into complex processing workflows\n",
    "3. **Implement Contextual Steps** - Steps that run within resource management contexts\n",
    "4. **Use Conditional Steps** - Execute steps only when specific conditions are met\n",
    "5. **Build Fittable Steps** - Two-phase ML workflows with separate fit/transform operations\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "- **Modularity**: Encapsulate processing logic in reusable components\n",
    "- **Composability**: Mix and match steps to build complex workflows\n",
    "- **ML-Ready**: Built-in support for fit/transform patterns\n",
    "- **Resource Management**: Automatic handling of contexts and cleanup\n",
    "\n",
    "---\n",
    "\n",
    "Let's start by setting up our environment and explore the different types of steps available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5eceed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b127d87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset created:\n",
      "\tShape: (10, 4)\n",
      "\tColumns: ['feature_1', 'feature_2', 'category', 'target']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2 category  target\n",
       "0        1.0        0.5        A       0\n",
       "1        2.0        1.5        B       1\n",
       "2        3.0        2.5        A       0\n",
       "3        4.0        3.5        B       1\n",
       "4        5.0        4.5        A       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "from src.idspy.core.step.base import Step\n",
    "from src.idspy.core.step.conditional import ConditionalStep\n",
    "from src.idspy.core.step.fittable import FittableStep\n",
    "from src.idspy.core.step.contextual import ContextualStep\n",
    "from src.idspy.core.pipeline.base import Pipeline\n",
    "from src.idspy.core.storage.dict import DictStorage\n",
    "\n",
    "# Create sample data for demonstrations\n",
    "data = pd.DataFrame({\n",
    "    \"feature_1\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0],\n",
    "    \"feature_2\": [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5],\n",
    "    \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\"],\n",
    "    \"target\": [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "})\n",
    "\n",
    "print(\"Sample dataset created:\")\n",
    "print(f\"\\tShape: {data.shape}\")\n",
    "print(f\"\\tColumns: {list(data.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87502315",
   "metadata": {},
   "source": [
    "## Base Step Implementation\n",
    "\n",
    "The **Step** class is the foundation of all processing components. Every step must implement:\n",
    "\n",
    "- **`@Step.needs()` Decorator**: Formally declare required inputs for step execution\n",
    "- **`bindings` Property**: Map logical parameter names to physical storage keys for flexible data routing\n",
    "- **`compute()`**: Contains the actual processing logic\n",
    "\n",
    "### Design Patterns\n",
    "\n",
    "- Steps declare their data dependencies explicitly through requirements\n",
    "- Bindings enable decoupling of step logic from storage implementation  \n",
    "- Required inputs are automatically injected as `compute()` method parameters\n",
    "- `Pipeline` and `Storage` are used to orchestrate steps and manage the flow of data between steps\n",
    "\n",
    "Let's create some example steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "082ae06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Data Loading Step\n",
    "class DataLoader(Step):\n",
    "    \"\"\"Loads data and makes it available to other steps.\"\"\"\n",
    "\n",
    "    def bindings(self):\n",
    "        return {\"dataset\": \"raw_data\"}  # Output key -> storage key\n",
    "\n",
    "    def compute(self):\n",
    "        # In practice, this might load from files, databases, APIs, etc.\n",
    "        print(\"Loading dataset...\")\n",
    "        return {\"dataset\": data.copy()}\n",
    "\n",
    "\n",
    "# Example 2: Feature Selection Step\n",
    "@Step.needs(\"data\") # Indicates this step needs some data provided by another step\n",
    "class FeatureSelector(Step):\n",
    "    \"\"\"Selects specific columns from the dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, columns):\n",
    "        super().__init__()\n",
    "        self.columns = columns\n",
    "\n",
    "    def bindings(self):\n",
    "        return {\"data\": \"raw_data\", \"selected\": \"selected_features\"}\n",
    "\n",
    "    def compute(self, data):\n",
    "        print(f\"Selecting columns: {self.columns}\")\n",
    "        selected = data[self.columns].copy()\n",
    "        print(f\"\\tOriginal shape: {data.shape} -> Selected shape: {selected.shape}\")\n",
    "        return {\"selected\": selected}\n",
    "\n",
    "\n",
    "# Example 3: Simple transformation step\n",
    "@Step.needs(\"data\")\n",
    "class ConstantAdder(Step):\n",
    "    \"\"\"Adds a constant value to numerical columns.\"\"\"\n",
    "\n",
    "    def __init__(self, value):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "\n",
    "    def bindings(self):\n",
    "        return {\"data\": \"selected_features\", \"result\": \"result\"}\n",
    "\n",
    "    def compute(self, data):\n",
    "        print(f\"Adding {self.value} to all numerical columns...\")\n",
    "        result = data.copy()\n",
    "        numeric_cols = result.select_dtypes(include=[np.number]).columns\n",
    "        result[numeric_cols] = result[numeric_cols] + self.value\n",
    "        return {\"result\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2333a9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Basic Steps\n",
      "========================================\n",
      "Loading dataset...\n",
      "Selecting columns: ['feature_1', 'feature_2']\n",
      "\tOriginal shape: (10, 4) -> Selected shape: (10, 2)\n",
      "Adding 100 to all numerical columns...\n",
      "\n",
      "Pipeline completed!\n",
      "\n",
      "Final transformed data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'result':    feature_1  feature_2\n",
       " 0      101.0      100.5\n",
       " 1      102.0      101.5\n",
       " 2      103.0      102.5\n",
       " 3      104.0      103.5\n",
       " 4      105.0      104.5\n",
       " 5      106.0      105.5\n",
       " 6      107.0      106.5\n",
       " 7      108.0      107.5\n",
       " 8      109.0      108.5\n",
       " 9      110.0      109.5}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the basic steps in a simple pipeline\n",
    "storage = DictStorage()\n",
    "\n",
    "loader = DataLoader()\n",
    "selector = FeatureSelector([\"feature_1\", \"feature_2\"])\n",
    "adder = ConstantAdder(100)\n",
    "\n",
    "print(\"Testing Basic Steps\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = Pipeline([loader, selector, adder], storage=storage)\n",
    "result = pipeline.run()\n",
    "\n",
    "print(f\"\\nPipeline completed!\")\n",
    "final_data = storage.get([\"result\"])\n",
    "print(f\"\\nFinal transformed data:\")\n",
    "display(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742f243",
   "metadata": {},
   "source": [
    "## Contextual Step Examples\n",
    "\n",
    "Steps that run within a context manager for automatic resource management and cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e77ff095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "class FileWriterStep(ContextualStep):\n",
    "    \"\"\"Contextual step that provides a temporary directory and writes data to a file.\"\"\"\n",
    "\n",
    "    def __init__(self, filename: str, content: str, name: Optional[str] = None):\n",
    "        super().__init__(name=name or \"FileWriterStep\")\n",
    "        self.filename = filename\n",
    "        self.content = content\n",
    "\n",
    "    @contextmanager\n",
    "    def context(self, **kwargs):\n",
    "        \"\"\"Create and cleanup a temporary directory.\"\"\"\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            print(f\"[{self.name}] Created temporary directory: {temp_dir}\")\n",
    "\n",
    "            # Provide context object with temp directory\n",
    "            ctx = type('Context', (), {'temp_dir': temp_dir})()\n",
    "\n",
    "            try:\n",
    "                yield ctx\n",
    "            finally:\n",
    "                print(f\"[{self.name}] Cleaned up temporary directory: {temp_dir}\")\n",
    "\n",
    "    def compute(self, context: Any = None, **kwargs: Any) -> Dict[str, Any]:\n",
    "        if context and hasattr(context, 'temp_dir'):\n",
    "            filepath = Path(context.temp_dir) / self.filename\n",
    "        else:\n",
    "            raise ValueError(\"Context with 'temp_dir' is required\")\n",
    "\n",
    "        print(f\"[{self.name}] Writing to: {filepath}\")\n",
    "\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(self.content)\n",
    "\n",
    "        return {\n",
    "            \"filepath\": str(filepath),\n",
    "            \"bytes_written\": len(self.content.encode())\n",
    "        }\n",
    "\n",
    "    def bindings(self) -> Dict[str, str]:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2144aed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Contextual Steps Demo ===\n",
      "[FileWriter] Created temporary directory: /var/folders/9q/5fdsccl51c34cl9rkc28dfx40000gn/T/tmphamx4k5y\n",
      "[FileWriter] Writing to: /var/folders/9q/5fdsccl51c34cl9rkc28dfx40000gn/T/tmphamx4k5y/example.txt\n",
      "[FileWriter] Cleaned up temporary directory: /var/folders/9q/5fdsccl51c34cl9rkc28dfx40000gn/T/tmphamx4k5y\n",
      "File written to: /var/folders/9q/5fdsccl51c34cl9rkc28dfx40000gn/T/tmphamx4k5y/example.txt\n",
      "Bytes written: 71\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate contextual steps\n",
    "print(\"=== Contextual Steps Demo ===\")\n",
    "\n",
    "# Create the wrapped step\n",
    "writer_step = FileWriterStep(name=\"FileWriter\", filename=\"example.txt\", content=\"Hello from contextual step!\\nThis file will be automatically cleaned up.\")\n",
    "\n",
    "# Run with automatic temp directory management\n",
    "result = writer_step.run()\n",
    "\n",
    "print(f\"File written to: {result['filepath']}\")\n",
    "print(f\"Bytes written: {result['bytes_written']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778087c0",
   "metadata": {},
   "source": [
    "## Conditional Step Examples\n",
    "\n",
    "Steps that execute only when specific conditions are met, with proper handling of skip scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a64bbe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityChecker(ConditionalStep):\n",
    "    \"\"\"Check if data meets quality requirements before processing.\"\"\"\n",
    "\n",
    "    def __init__(self, min_rows: int = 5, required_columns: list = None, name: Optional[str] = None):\n",
    "        super().__init__(name=name)\n",
    "        self.min_rows = min_rows\n",
    "        self.required_columns = required_columns or []\n",
    "\n",
    "    def bindings(self) -> Dict[str, str]:\n",
    "        return {}\n",
    "\n",
    "    def should_run(self, data: pd.DataFrame, **kwargs) -> bool:\n",
    "        has_enough_rows = len(data) >= self.min_rows\n",
    "        has_required_columns = all(col in data.columns for col in self.required_columns)\n",
    "\n",
    "        print(f\"[{self.name}] Quality check - Rows: {len(data)}>={self.min_rows}: {has_enough_rows}\")\n",
    "        print(f\"[{self.name}] Quality check - Required columns {self.required_columns}: {has_required_columns}\")\n",
    "\n",
    "        return has_enough_rows and has_required_columns\n",
    "\n",
    "    def on_skip(self, data: pd.DataFrame, **kwargs) -> None:\n",
    "        print(f\"[{self.name}] SKIPPED - Data quality check failed\")\n",
    "        print(f\"  - Rows: {len(data)} (need >= {self.min_rows})\")\n",
    "        print(f\"  - Missing columns: {set(self.required_columns) - set(data.columns)}\")\n",
    "\n",
    "    def compute(self, **kwargs: Any) -> Dict[str, Any]:\n",
    "        print(f\"[{self.name}] Data quality check PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d363c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataQualityChecker] Quality check - Rows: 10>=5: True\n",
      "[DataQualityChecker] Quality check - Required columns ['feature_1', 'target']: True\n",
      "[DataQualityChecker] Data quality check PASSED\n",
      "[DataQualityChecker] Quality check - Rows: 3>=5: False\n",
      "[DataQualityChecker] Quality check - Required columns ['feature_1', 'target']: True\n",
      "[DataQualityChecker] SKIPPED - Data quality check failed\n",
      "  - Rows: 3 (need >= 5)\n",
      "  - Missing columns: set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with good data\n",
    "good_checker = DataQualityChecker(min_rows=5, required_columns=[\"feature_1\", \"target\"])\n",
    "good_checker.run(data=data)\n",
    "\n",
    "# Test with insufficient data\n",
    "small_data = data.head(3)  # Only 3 rows\n",
    "bad_checker = DataQualityChecker(min_rows=5, required_columns=[\"feature_1\", \"target\"])\n",
    "bad_checker.run(data=small_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c6bb3",
   "metadata": {},
   "source": [
    "### Fittable Step Implementation\n",
    "\n",
    "Two-phase ML workflow with separate fitting and execution phases, demonstrating proper state management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fc03d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler(FittableStep):\n",
    "    \"\"\"Standardize features by removing mean and scaling to unit variance.\"\"\"\n",
    "\n",
    "    def __init__(self, features: list = None, name: Optional[str] = None):\n",
    "        super().__init__(name=name)\n",
    "        self.features = features\n",
    "        self.mean_ = None\n",
    "        self.std_ = None\n",
    "\n",
    "    @property\n",
    "    def bindings(self) -> Dict[str, str]:\n",
    "        return {}\n",
    "\n",
    "    def fit_impl(self, data: pd.DataFrame, **kwargs) -> None:\n",
    "        \"\"\"Learn the mean and standard deviation from training data.\"\"\"\n",
    "        features = self.features or data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        print(f\"[{self.name}] Fitting on features: {features}\")\n",
    "        self.mean_ = data[features].mean()\n",
    "        self.std_ = data[features].std()\n",
    "\n",
    "        print(f\"[{self.name}] Learned statistics:\")\n",
    "        print(f\"  Mean: {dict(self.mean_)}\")\n",
    "        print(f\"  Std: {dict(self.std_)}\")\n",
    "\n",
    "    def compute(self, data: pd.DataFrame, **kwargs: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Apply learned scaling to new data.\"\"\"\n",
    "        features = self.features or data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "        print(f\"[{self.name}] Applying scaling to {len(features)} features\")\n",
    "        scaled_data = data.copy()\n",
    "        scaled_data[features] = (data[features] - self.mean_[features]) / self.std_[features]\n",
    "\n",
    "        return {\"scaled_data\": scaled_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d16783b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fittable Steps Demo ===\n",
      "Train data shape: (7, 4)\n",
      "Test data shape: (3, 4)\n",
      "\n",
      "Scaler fitted: False\n",
      "Expected error: Step StandardScaler(, fitted=False) is not fitted; cannot run.\n",
      "\n",
      "[StandardScaler] Fitting on features: ['feature_1', 'feature_2']\n",
      "[StandardScaler] Learned statistics:\n",
      "  Mean: {'feature_1': np.float64(4.0), 'feature_2': np.float64(3.5)}\n",
      "  Std: {'feature_1': np.float64(2.160246899469287), 'feature_2': np.float64(2.160246899469287)}\n",
      "[StandardScaler] Applying scaling to 2 features\n",
      "\n",
      "Original test data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2 category  target\n",
       "7        8.0        7.5        B       1\n",
       "8        9.0        8.5        A       0\n",
       "9       10.0        9.5        B       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled test data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.85164</td>\n",
       "      <td>1.85164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.31455</td>\n",
       "      <td>2.31455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.77746</td>\n",
       "      <td>2.77746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2\n",
       "7    1.85164    1.85164\n",
       "8    2.31455    2.31455\n",
       "9    2.77746    2.77746"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Demonstrate fittable steps\n",
    "print(\"=== Fittable Steps Demo ===\")\n",
    "\n",
    "# Create train/test split\n",
    "train_data = data.iloc[:7]  # First 7 rows for training\n",
    "test_data = data.iloc[7:]   # Last 3 rows for testing\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "\n",
    "# Create and fit scaler\n",
    "scaler = StandardScaler(features=[\"feature_1\", \"feature_2\"])\n",
    "print(f\"\\nScaler fitted: {scaler.is_fitted}\")\n",
    "\n",
    "# Try to run without fitting (should raise error)\n",
    "try:\n",
    "    scaler.run(data=test_data)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Expected error: {e}\\n\")\n",
    "\n",
    "# Fit the scaler\n",
    "scaler.fit(data=train_data)\n",
    "\n",
    "# Now run successfully\n",
    "scaled_result = scaler.run(data=test_data)\n",
    "print(f\"\\nOriginal test data:\")\n",
    "display(test_data)\n",
    "print(f\"\\nScaled test data:\")\n",
    "display(scaled_result['scaled_data'][['feature_1', 'feature_2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f929dc",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Step Architecture**: Steps are reusable components with clear input/output contracts\n",
    "2. **Base Step Pattern**: Implement `bindings()`, `compute()`, and `@needs()`\n",
    "3. **Composability**: Steps can be combined into complex processing pipelines\n",
    "4. **Resource Management**: ContextualStep handles setup/teardown automatically\n",
    "5. **Conditional Execution**: ConditionalStep allows steps to run only when conditions are met\n",
    "6. **ML Workflows**: FittableStep provides fit/transform patterns for machine learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
