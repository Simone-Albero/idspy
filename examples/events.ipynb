{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24043496",
   "metadata": {},
   "source": [
    "# Event-Driven Architecture with idspy\n",
    "\n",
    "This notebook demonstrates idspy's powerful event-driven system for pipeline monitoring, debugging, and observability. Events provide a clean way to track what happens during steps orchestration.\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "In this tutorial, you'll discover how to:\n",
    "\n",
    "1. **Create and Use Events** - Immutable event objects with structured data\n",
    "2. **Filter Events** - Use predicates to match and select specific events\n",
    "3. **Event Bus System** - Centralized event subscription and dispatching \n",
    "4. **Event Handlers** - Structured event processing with custom logic\n",
    "5. **Observable Pipelines** - Automatic event emission during pipeline execution\n",
    "6. **Error Tracking** - Event-driven error handling and diagnostics\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "- **Debugging Made Easy**: Track exactly what happens in your pipelines\n",
    "- **Flexible Monitoring**: Subscribe to specific events you care about\n",
    "- **Decoupled Architecture**: Separate business logic from monitoring concerns\n",
    "- **Real-time Insights**: Process events as they happen\n",
    "\n",
    "---\n",
    "\n",
    "Let's start by setting up our environment and exploring the core event system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a8fcc8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b91e35df280f80fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T18:24:07.476641Z",
     "start_time": "2025-09-03T18:24:07.472412Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.idspy.core.pipeline.base import PipelineEvent\n",
    "from src.idspy.core.pipeline.observable import ObservablePipeline\n",
    "from src.idspy.core.step.base import Step\n",
    "from src.idspy.core.events.event import Event, only_source, source_startswith, has_payload_key, payload_key_equals\n",
    "from src.idspy.core.events.bus import EventBus\n",
    "from src.idspy.core.events.handler import BaseHandler\n",
    "from src.idspy.core.storage.dict import DictStorage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9446ea",
   "metadata": {},
   "source": [
    "## Understanding Events\n",
    "\n",
    "Events are **immutable** objects that represent something that happened in your system. Each event contains:\n",
    "\n",
    "- **`type`**: What kind of event occurred (e.g., \"step_start\", \"error\", \"validation\")\n",
    "- **`source`**: Where the event came from (e.g., \"DataProcessor.Validation\") \n",
    "- **`payload`**: Additional data about the event (flexible dictionary)\n",
    "\n",
    "Let's create our first event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cb848ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event created successfully!\n",
      "Type: pipeline_step\n",
      "Source: DataProcessor.Validation\n",
      "Payload keys: ['raw_data', 'memory_usage', 'processing_time', 'status']\n",
      "\n",
      "Full event as dict:\n",
      "{'type': 'pipeline_step', 'source': 'DataProcessor.Validation', 'payload': {'raw_data': {'rows': 10000, 'cols': 15}, 'memory_usage': '256MB', 'processing_time': 1.23, 'status': 'success'}, 'timestamp': '2025-10-07T17:01:01.820838+00:00'}\n"
     ]
    }
   ],
   "source": [
    "# Create an event representing a pipeline step completion\n",
    "event = Event(\n",
    "    type=\"pipeline_step\",\n",
    "    source=\"DataProcessor.Validation\",\n",
    "    payload={\n",
    "        \"raw_data\": {\"rows\": 10000, \"cols\": 15},\n",
    "        \"memory_usage\": \"256MB\",\n",
    "        \"processing_time\": 1.23,\n",
    "        \"status\": \"success\"\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Event created successfully!\")\n",
    "print(f\"Type: {event.type}\")\n",
    "print(f\"Source: {event.source}\")\n",
    "print(f\"Payload keys: {list(event.payload.keys())}\")\n",
    "\n",
    "# Events are immutable - this would raise an error:\n",
    "# event.type = \"different_type\"  # AttributeError!\n",
    "\n",
    "print(f\"\\nFull event as dict:\")\n",
    "print(event.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc11838",
   "metadata": {},
   "source": [
    "## Event Predicates - Smart Filtering\n",
    "\n",
    "Predicates are functions that help you filter events based on specific criteria. idspy provides several built-in predicates for common filtering tasks:\n",
    "\n",
    "- **`only_source(source)`**: Match events from a specific source\n",
    "- **`source_startswith(prefix)`**: Match events whose source starts with a prefix  \n",
    "- **`has_payload_key(key)`**: Match events that have a specific key in their payload\n",
    "- **`payload_key_equals(key, value)`**: Match events where payload[key] equals a specific value\n",
    "\n",
    "Let's see these in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8200bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Predicate Examples\n",
      "==================================================\n",
      "Events from 'Pipeline.DataLoad' (3 found):\n",
      "   • step_start - N/A\n",
      "   • step_end - N/A\n",
      "   • warning - Large file size\n",
      "\n",
      "Events from 'Pipeline.*' sources (5 found):\n",
      "   • step_start from Pipeline.DataLoad\n",
      "   • step_start from Pipeline.DataTransform\n",
      "   • step_end from Pipeline.DataLoad\n",
      "   • step_end from Pipeline.DataTransform\n",
      "   • warning from Pipeline.DataLoad\n",
      "\n",
      "Events with 'duration' payload (2 found):\n",
      "   • Pipeline.DataLoad: 1.5s\n",
      "   • Pipeline.DataTransform: 3.2s\n",
      "\n",
      "Events with type='input' (1 found):\n",
      "   • Pipeline.DataLoad: 150MB\n"
     ]
    }
   ],
   "source": [
    "# Create a diverse set of test events\n",
    "test_events = [\n",
    "    Event(\"step_start\", \"Pipeline.DataLoad\", payload={\"step_index\": 0, \"type\": \"input\", \"size_mb\": 150}),\n",
    "    Event(\"step_start\", \"Pipeline.DataTransform\", payload={\"step_index\": 1, \"type\": \"processing\"}),\n",
    "    Event(\"step_end\", \"Pipeline.DataLoad\", payload={\"step_index\": 0, \"duration\": 1.5, \"status\": \"success\"}),\n",
    "    Event(\"step_end\", \"Pipeline.DataTransform\", payload={\"step_index\": 1, \"duration\": 3.2, \"status\": \"success\"}),\n",
    "    Event(\"error\", \"OtherPipeline.Validation\", payload={\"error_code\": 404, \"message\": \"Data not found\"}),\n",
    "    Event(\"warning\", \"Pipeline.DataLoad\", payload={\"message\": \"Large file size\", \"size_mb\": 800}),\n",
    "]\n",
    "\n",
    "print(\"Event Predicate Examples\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Filter by exact source\n",
    "load_predicate = only_source(\"Pipeline.DataLoad\")\n",
    "load_events = list(filter(load_predicate, test_events))\n",
    "print(f\"Events from 'Pipeline.DataLoad' ({len(load_events)} found):\")\n",
    "for event in load_events:\n",
    "    print(f\"   • {event.type} - {event.payload.get('message', 'N/A')}\")\n",
    "\n",
    "# 2. Filter by source prefix\n",
    "pipeline_predicate = source_startswith(\"Pipeline.\")\n",
    "pipeline_events = list(filter(pipeline_predicate, test_events))\n",
    "print(f\"\\nEvents from 'Pipeline.*' sources ({len(pipeline_events)} found):\")\n",
    "for event in pipeline_events:\n",
    "    print(f\"   • {event.type} from {event.source}\")\n",
    "\n",
    "# 3. Filter by payload key existence\n",
    "has_duration = has_payload_key(\"duration\")\n",
    "duration_events = list(filter(has_duration, test_events))\n",
    "print(f\"\\nEvents with 'duration' payload ({len(duration_events)} found):\")\n",
    "for event in duration_events:\n",
    "    print(f\"   • {event.source}: {event.payload['duration']}s\")\n",
    "\n",
    "# 4. Filter by payload value\n",
    "input_type = payload_key_equals(\"type\", \"input\")\n",
    "input_events = list(filter(input_type, test_events))\n",
    "print(f\"\\nEvents with type='input' ({len(input_events)} found):\")\n",
    "for event in input_events:\n",
    "    print(f\"   • {event.source}: {event.payload.get('size_mb', 'N/A')}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec82350ec72c07",
   "metadata": {},
   "source": [
    "## EventBus - Central Event Hub\n",
    "\n",
    "The **EventBus** is the heart of the event system. It manages subscriptions and dispatches events to interested handlers. Think of it as a sophisticated messaging system where:\n",
    "\n",
    "- **Publishers** emit events to the bus\n",
    "- **Subscribers** register to receive specific events\n",
    "- **Priority system** controls handler execution order\n",
    "- **Filtering** ensures handlers only receive relevant events\n",
    "\n",
    "Let's explore the EventBus capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a1d98600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventBus Subscription Patterns\n",
      "==================================================\n",
      "\n",
      "Testing Event Emission:\n",
      "------------------------------\n",
      "\n",
      "Emitting: user_action from WebInterface\n",
      "  [USER_HANDLER] Action: login\n",
      "  [GLOBAL] user_action from WebInterface\n",
      "\n",
      "Emitting: error from DatabaseConnection\n",
      "  [ERROR_HANDLER] Connection timeout\n",
      "  [GLOBAL] error from DatabaseConnection\n",
      "\n",
      "Emitting: step_start from DataPipeline.LoadData\n",
      "  [GLOBAL] step_start from DataPipeline.LoadData\n",
      "\n",
      "Emitting: notification from EmailService\n",
      "  [GLOBAL] notification from EmailService\n"
     ]
    }
   ],
   "source": [
    "# Create an EventBus for demonstration\n",
    "demo_bus = EventBus()\n",
    "\n",
    "print(\"EventBus Subscription Patterns\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pattern 1: Function-based handlers with decorators\n",
    "@demo_bus.on()  # Subscribe to ALL events\n",
    "def global_logger(event: Event) -> None:\n",
    "    print(f\"  [GLOBAL] {event.type} from {event.source}\")\n",
    "\n",
    "@demo_bus.on(\"user_action\")  # Subscribe to specific event type\n",
    "def user_action_handler(event: Event) -> None:\n",
    "    print(f\"  [USER_HANDLER] Action: {event.payload.get('action', 'unknown')}\")\n",
    "\n",
    "@demo_bus.on(\"error\")  # Subscribe to error events\n",
    "def error_handler(event: Event) -> None:\n",
    "    print(f\"  [ERROR_HANDLER] {event.payload.get('message', 'No error message')}\")\n",
    "\n",
    "# Pattern 2: Predicate-based subscription (more flexible)\n",
    "@demo_bus.on(source_startswith(\"DataPipeline.\"))\n",
    "def pipeline_monitor(event: Event) -> None:\n",
    "    print(f\"  [PIPELINE_MONITOR] {event.source} -> {event.type}\")\n",
    "\n",
    "# Let's test by emitting some events\n",
    "print(\"\\nTesting Event Emission:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "events = [\n",
    "    Event(\"user_action\", \"WebInterface\", payload={\"action\": \"login\", \"user_id\": 123}),\n",
    "    Event(\"error\", \"DatabaseConnection\", payload={\"message\": \"Connection timeout\"}),\n",
    "    Event(\"step_start\", \"DataPipeline.LoadData\", payload={\"step\": \"data_loading\"}),\n",
    "    Event(\"notification\", \"EmailService\", payload={\"recipient\": \"admin@example.com\"}),\n",
    "]\n",
    "\n",
    "for event in events:\n",
    "    print(f\"\\nEmitting: {event.type} from {event.source}\")\n",
    "    demo_bus.publish(event_type=event.type, source=event.source, payload=event.payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33fd559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subscription Management:\n",
      "------------------------------\n",
      "Unsubscribing global logger...\n",
      "\n",
      "Emitting test event after unsubscription:\n",
      "  [USER_HANDLER] Action: logout\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate subscription management\n",
    "print(\"\\nSubscription Management:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Unsubscribe the global logger (it was getting noisy!)\n",
    "# Note: In practice, you'd store the token when subscribing\n",
    "print(\"Unsubscribing global logger...\")\n",
    "demo_bus.unsubscribe(1)\n",
    "\n",
    "# Test with remaining handlers\n",
    "test_event = Event(\"user_action\", \"MobileApp\", payload={\"action\": \"logout\", \"user_id\": 456})\n",
    "print(f\"\\nEmitting test event after unsubscription:\")\n",
    "demo_bus.publish(event_type=test_event.type, source=test_event.source, payload=test_event.payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e085a79",
   "metadata": {},
   "source": [
    "## BaseHandler - Structured Event Processing\n",
    "\n",
    "While function-based handlers are simple, **BaseHandler** provides more structure for complex event processing. It offers:\n",
    "\n",
    "- **`can_handle(event)`**: Filtering logic to determine if handler should process an event\n",
    "- **`handle(event)`**: The actual processing logic\n",
    "- **State management**: Handlers can maintain internal state\n",
    "- **Object-oriented design**: Better organization for complex handlers\n",
    "\n",
    "Let's create a sophisticated pipeline monitoring handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "78c29a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PipelineMonitor Handler\n",
      "==================================================\n",
      "[DataProcessing] Pipeline started\n",
      "Step 1: LoadData starting...\n",
      "Step 1: LoadData completed (2.1s, success)\n",
      "Step 2: ValidateData starting...\n",
      "Error in ValidateData: Missing required column 'target'\n",
      "Step 2: ValidateData completed (0.5s, failed)\n",
      "Step 3: CleanData starting...\n",
      "Step 3: CleanData completed (1.8s, success)\n",
      "[DataProcessing] Pipeline completed in 4.4s\n",
      "Total steps: 3, Errors: 1\n"
     ]
    }
   ],
   "source": [
    "# Create a sophisticated pipeline monitoring handler\n",
    "class PipelineMonitor(BaseHandler):\n",
    "    \"\"\"Monitors pipeline execution with state tracking and detailed logging.\"\"\"\n",
    "\n",
    "    def __init__(self, pipeline_name: str):\n",
    "        super().__init__()\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.current_step = None\n",
    "        self.step_count = 0\n",
    "        self.start_time = None\n",
    "        self.errors = []\n",
    "\n",
    "    def can_handle(self, event: Event) -> bool:\n",
    "        \"\"\"Only handle events from our specific pipeline.\"\"\"\n",
    "        return event.source.startswith(f\"{self.pipeline_name}.\")\n",
    "\n",
    "    def handle(self, event: Event) -> None:\n",
    "        \"\"\"Process pipeline events with state tracking.\"\"\"\n",
    "        if event.type == \"pipeline_start\":\n",
    "            print(f\"[{self.pipeline_name}] Pipeline started\")\n",
    "            self.step_count = 0\n",
    "            self.start_time = event.payload.get(\"timestamp\")\n",
    "\n",
    "        elif event.type == \"step_start\":\n",
    "            self.current_step = event.source.split(\".\")[-1]  # Get step name\n",
    "            self.step_count += 1\n",
    "            print(f\"Step {self.step_count}: {self.current_step} starting...\")\n",
    "\n",
    "        elif event.type == \"step_end\":\n",
    "            duration = event.payload.get(\"duration\", \"unknown\")\n",
    "            status = event.payload.get(\"status\", \"unknown\")\n",
    "            print(f\"Step {self.step_count}: {self.current_step} completed ({duration}s, {status})\")\n",
    "\n",
    "        elif event.type == \"error\":\n",
    "            error_msg = event.payload.get(\"message\", \"Unknown error\")\n",
    "            self.errors.append(error_msg)\n",
    "            print(f\"Error in {self.current_step}: {error_msg}\")\n",
    "\n",
    "        elif event.type == \"pipeline_end\":\n",
    "            total_duration = event.payload.get(\"total_duration\", \"unknown\")\n",
    "            print(f\"[{self.pipeline_name}] Pipeline completed in {total_duration}s\")\n",
    "            print(f\"Total steps: {self.step_count}, Errors: {len(self.errors)}\")\n",
    "\n",
    "# Create and register the handler\n",
    "handler_bus = EventBus()\n",
    "pipeline_monitor = PipelineMonitor(\"DataProcessing\")\n",
    "handler_bus.subscribe(pipeline_monitor)\n",
    "\n",
    "print(\"Testing PipelineMonitor Handler\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate a complete pipeline execution\n",
    "pipeline_events = [\n",
    "    Event(\"pipeline_start\", \"DataProcessing.Start\", payload={\"timestamp\": \"2024-01-15T10:00:00\"}),\n",
    "    Event(\"step_start\", \"DataProcessing.LoadData\", payload={\"step_index\": 1}),\n",
    "    Event(\"step_end\", \"DataProcessing.LoadData\", payload={\"step_index\": 1, \"duration\": 2.1, \"status\": \"success\"}),\n",
    "    Event(\"step_start\", \"DataProcessing.ValidateData\", payload={\"step_index\": 2}),\n",
    "    Event(\"error\", \"DataProcessing.ValidateData\", payload={\"message\": \"Missing required column 'target'\"}),\n",
    "    Event(\"step_end\", \"DataProcessing.ValidateData\", payload={\"step_index\": 2, \"duration\": 0.5, \"status\": \"failed\"}),\n",
    "    Event(\"step_start\", \"DataProcessing.CleanData\", payload={\"step_index\": 3}),\n",
    "    Event(\"step_end\", \"DataProcessing.CleanData\", payload={\"step_index\": 3, \"duration\": 1.8, \"status\": \"success\"}),\n",
    "    Event(\"pipeline_end\", \"DataProcessing.End\", payload={\"total_duration\": 4.4}),\n",
    "]\n",
    "\n",
    "# Process all events\n",
    "for event in pipeline_events:\n",
    "    handler_bus.publish(event_type=event.type, source=event.source, payload=event.payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d57951",
   "metadata": {},
   "source": [
    "## ObservablePipeline\n",
    "\n",
    "Pipeline integration with automatic event emission for monitoring and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c2df733328d40",
   "metadata": {},
   "source": [
    "### Example Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bed906bcd8ad3a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T18:24:07.504436Z",
     "start_time": "2025-09-03T18:24:07.500944Z"
    }
   },
   "outputs": [],
   "source": [
    "class Load(Step):\n",
    "    def bindings(self):\n",
    "        return {\"data\": \"data\"}\n",
    "\n",
    "    def compute(self):\n",
    "        return {\"data\": [1, 2, 3]}\n",
    "\n",
    "@Step.needs(\"data\")\n",
    "class Sum(Step):\n",
    "    def bindings(self):\n",
    "        return {\"data\": \"data\", \"sum\": \"sum\"}\n",
    "\n",
    "    def compute(self, data):\n",
    "        return {\"sum\": sum(data)}\n",
    "\n",
    "class Boom(Step):\n",
    "    def bindings(self):\n",
    "        return {}\n",
    "\n",
    "    def compute(self):\n",
    "        raise RuntimeError(\"Boom!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff07650a76952",
   "metadata": {},
   "source": [
    "### Observable Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9b15820328ca9184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T18:24:07.513086Z",
     "start_time": "2025-09-03T18:24:07.510094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ObservablePipeline Demo ===\n",
      "[GLOBAL] -> Demo\n",
      "[PipelineEvent.PIPELINE_START] -> Demo\n",
      "[GLOBAL] -> Demo.Load\n",
      "[PipelineEvent.STEP_START] -> Demo.Load\n",
      "[GLOBAL] -> Demo.Load\n",
      "[PipelineEvent.STEP_END] -> Demo.Load\n",
      "[GLOBAL] -> Demo.Sum\n",
      "[PipelineEvent.STEP_START] -> Demo.Sum\n",
      "[GLOBAL] -> Demo.Sum\n",
      "[PipelineEvent.STEP_END] -> Demo.Sum\n",
      "[GLOBAL] -> Demo\n",
      "[PipelineEvent.PIPELINE_END] -> Demo\n",
      "\n",
      "Final STATE: {'data': [1, 2, 3], 'sum': 6}\n"
     ]
    }
   ],
   "source": [
    "# Create and run an ObservablePipeline\n",
    "bus = EventBus()\n",
    "@bus.on(priority=0)\n",
    "def global_logger(event: Event) -> None:\n",
    "    print(f\"[GLOBAL] -> {event.source}\")\n",
    "\n",
    "@bus.on(PipelineEvent.PIPELINE_START)\n",
    "def global_logger(event: Event) -> None:\n",
    "    print(f\"[{event.type}] -> {event.source}\")\n",
    "\n",
    "@bus.on(PipelineEvent.PIPELINE_END)\n",
    "def global_logger(event: Event) -> None:\n",
    "    print(f\"[{event.type}] -> {event.source}\")\n",
    "\n",
    "@bus.on(PipelineEvent.STEP_START)\n",
    "def global_logger(event: Event) -> None:\n",
    "    print(f\"[{event.type}] -> {event.source}\")\n",
    "\n",
    "@bus.on(PipelineEvent.STEP_END)\n",
    "def global_logger(event: Event) -> None:\n",
    "    print(f\"[{event.type}] -> {event.source}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"=== ObservablePipeline Demo ===\")\n",
    "storage = DictStorage()\n",
    "pipeline = ObservablePipeline(steps=[Load(), Sum()], bus=bus, name=\"Demo\", storage=storage)\n",
    "pipeline.run()\n",
    "\n",
    "print(f\"\\nFinal STATE: {storage.as_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e250c69aa80ea",
   "metadata": {},
   "source": [
    "### Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6d041f17ee1430d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T18:24:07.527943Z",
     "start_time": "2025-09-03T18:24:07.525462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GLOBAL] -> ErrDemo\n",
      "[PipelineEvent.PIPELINE_START] -> ErrDemo\n",
      "[GLOBAL] -> ErrDemo.Boom\n",
      "[PipelineEvent.STEP_START] -> ErrDemo.Boom\n",
      "[GLOBAL] -> ErrDemo.Boom\n",
      "[ERROR] -> Boom!\n"
     ]
    }
   ],
   "source": [
    "@bus.on(PipelineEvent.STEP_ERROR)\n",
    "def global_logger(event: Event) -> None:\n",
    "    print(f\"[ERROR] -> {event.payload.get('error')}\")\n",
    "\n",
    "pipeline = ObservablePipeline([Boom()], name=\"ErrDemo\", bus=bus, storage=DictStorage())\n",
    "\n",
    "try:\n",
    "    pipeline.run()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6abfb7d",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Event Structure**: Events are immutable objects with type, source, payload, and unique ID\n",
    "2. **Event Predicates**: Powerful filtering functions for selecting specific events\n",
    "3. **EventBus**: Central hub for event subscription, dispatching, and management\n",
    "4. **Handler Patterns**: Both function-based and class-based handlers for different use cases\n",
    "5. **Pipeline Integration**: Events provide excellent observability for data processing workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
