{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0aafc9ceb99e5e8",
   "metadata": {},
   "source": [
    "# Pipeline Architecture with idspy\n",
    "\n",
    "This notebook demonstrates how to build and customize pipelines. Pipelines orchestrate sequences of steps with built-in event handling, conditional execution, and ML-specific features.\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "In this tutorial, you'll discover how to:\n",
    "\n",
    "1. **Build Basic Pipelines** - Orchestrate sequences of steps\n",
    "2. **Create Custom Steps** - Define reusable components\n",
    "3. **Handle Events** - Add monitoring and debugging capabilities  \n",
    "4. **Use Fittable Pipelines** - ML workflows with separate fit/transform phases\n",
    "5. **Priority Systems** - Control execution order with multi-priority hooks\n",
    "6. **Repeatable Pipelines** - Conditional re-execution based on storage state\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "- **Modular Design**: Compose complex workflows from simple steps\n",
    "- **Event Integration**: Built-in observability and debugging\n",
    "- **ML-Ready**: Fittable pipelines for training/inference workflows\n",
    "- **Flexible Execution**: Conditional and repeatable pipeline patterns\n",
    "- **State Management**: Automatic storage integration for persistence\n",
    "\n",
    "---\n",
    "\n",
    "Let's start by setting up our environment and building our first pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9008cdd4332453d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T16:15:22.248593Z",
     "start_time": "2025-09-03T16:15:22.235873Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.idspy.core.storage.dict import DictStorage\n",
    "from src.idspy.core.step.base import Step\n",
    "from src.idspy.core.step.fittable import FittableStep\n",
    "from src.idspy.core.pipeline.base import Pipeline, PipelineEvent\n",
    "from src.idspy.core.pipeline.fittable import FittablePipeline\n",
    "from src.idspy.core.pipeline.observable import ObservablePipeline\n",
    "from src.idspy.core.pipeline.repeatable import RepeatablePipeline, StoragePredicate\n",
    "from src.idspy.core.events.bus import EventBus\n",
    "from src.idspy.core.events.event import Event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d048dd1a556d0dd",
   "metadata": {},
   "source": [
    "## Building Blocks - Example Steps\n",
    "\n",
    "Before building pipelines, let's define some example processing steps. Each step declares its input/output requirements through the `bindings()` method and implements its logic in `compute()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "449c9925ad8d96b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T16:15:22.255836Z",
     "start_time": "2025-09-03T16:15:22.251582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define example steps for our pipelines\n",
    "class Load(Step):\n",
    "    \"\"\"Loads initial data.\"\"\"\n",
    "    def bindings(self):\n",
    "        return {\"data\": \"data\"}\n",
    "\n",
    "    def compute(self):\n",
    "        # Simulate loading some data\n",
    "        return {\"data\": [1, 2, 3, 4, 5]}\n",
    "\n",
    "@Step.needs(\"data\")\n",
    "class Sum(Step):\n",
    "    \"\"\"Calculates sum of input data.\"\"\"\n",
    "    def bindings(self):\n",
    "        return {\"sum\": \"sum\", \"data\": \"data\"}\n",
    "\n",
    "    def compute(self, data):\n",
    "        total = sum(data)\n",
    "        print(f\"  Sum of {data} = {total}\")\n",
    "        return {\"sum\": total}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b656ebe8a6654",
   "metadata": {},
   "source": [
    "## Basic Pipeline Execution\n",
    "\n",
    "Let's start with a simple pipeline that loads data and calculates its sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fe3f8d5f04849fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T16:15:22.336616Z",
     "start_time": "2025-09-03T16:15:22.333258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Basic Pipeline\n",
      "========================================\n",
      "  Sum of [1, 2, 3, 4, 5] = 15\n",
      "Pipeline result: {'data': [1, 2, 3, 4, 5], 'sum': 15}\n",
      "\n",
      "The pipeline automatically manages data flow between steps!\n"
     ]
    }
   ],
   "source": [
    "# Create and run a basic pipeline\n",
    "storage = DictStorage()\n",
    "pipeline = Pipeline([Load(), Sum()], name=\"BasicPipeline\", storage=storage)\n",
    "\n",
    "print(\"Running Basic Pipeline\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "result = pipeline.run()\n",
    "\n",
    "print(f\"Pipeline result: {result}\")\n",
    "\n",
    "print(\"\\nThe pipeline automatically manages data flow between steps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880e534",
   "metadata": {},
   "source": [
    "## Custom Pipeline with Event Hooks\n",
    "\n",
    "You can create custom pipeline classes that respond to lifecycle events. This is perfect for adding logging, monitoring, or custom behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "767f47e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Custom Pipeline with Event Hooks\n",
      "==================================================\n",
      "[PIPELINE] Starting custom pipeline execution!\n",
      "[STEP 1] Starting: Load\n",
      "\tInputs: []\n",
      "[STEP 1] Completed: Load\n",
      "\tOutputs: ['data']\n",
      "[STEP 2] Starting: Sum\n",
      "\tInputs: ['data']\n",
      "  Sum of [1, 2, 3, 4, 5] = 15\n",
      "[STEP 2] Completed: Sum\n",
      "\tOutputs: ['sum']\n",
      "[PIPELINE] Custom pipeline execution completed!\n",
      "\tFinal result keys: ['data', 'sum']\n",
      "\n",
      "Final storage state: {'data': [1, 2, 3, 4, 5], 'sum': 15}\n"
     ]
    }
   ],
   "source": [
    "# Create a custom pipeline with event hooks\n",
    "class MyPipeline(Pipeline):\n",
    "    @Pipeline.hook(PipelineEvent.PIPELINE_START)\n",
    "    def _start(self) -> None:\n",
    "        print(\"[PIPELINE] Starting custom pipeline execution!\")\n",
    "\n",
    "    @Pipeline.hook(PipelineEvent.STEP_START)\n",
    "    def _before_step(self, step: Step, index: int, inputs: dict) -> None:\n",
    "        print(f\"[STEP {index+1}] Starting: {step.__class__.__name__}\")\n",
    "        print(f\"\\tInputs: {list(inputs.keys())}\")\n",
    "\n",
    "    @Pipeline.hook(PipelineEvent.STEP_END)\n",
    "    def _after_step(self, step: Step, index: int, outputs: dict) -> None:\n",
    "        print(f\"[STEP {index+1}] Completed: {step.__class__.__name__}\")\n",
    "        print(f\"\\tOutputs: {list(outputs.keys())}\")\n",
    "\n",
    "    @Pipeline.hook(PipelineEvent.PIPELINE_END)\n",
    "    def _finish(self, result: dict) -> None:\n",
    "        print(\"[PIPELINE] Custom pipeline execution completed!\")\n",
    "        print(f\"\\tFinal result keys: {list(result.keys())}\")\n",
    "\n",
    "# Test the custom pipeline\n",
    "print(\"Testing Custom Pipeline with Event Hooks\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "storage = DictStorage()\n",
    "custom_pipeline = MyPipeline([Load(), Sum()], name=\"CustomPipeline\", storage=storage)\n",
    "custom_pipeline.run()\n",
    "\n",
    "print(f\"\\nFinal storage state: {storage.as_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea4eb9854e4692",
   "metadata": {},
   "source": [
    "## Fittable Pipelines\n",
    "\n",
    "Automatic fitting lifecycle for ML workflows. Controls refit behavior across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09963d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Step.needs(\"data\")\n",
    "class MeanCenter(FittableStep):\n",
    "    \"\"\"Centers data by subtracting the mean.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(self)\n",
    "        self.mean = None\n",
    "\n",
    "    def bindings(self):\n",
    "        return {\"data\": \"data\", \"centered_data\": \"centered_data\"}\n",
    "\n",
    "    def fit_impl(self, data):\n",
    "        self.mean = sum(data) / len(data)\n",
    "        print(f\"  Fitted mean: {self.mean}\")\n",
    "        return {\"mean\": self.mean}\n",
    "\n",
    "    def compute(self, data):\n",
    "        centered = [x - self.mean for x in data]\n",
    "        print(f\"  Centered data: {centered}\")\n",
    "        return {\"centered_data\": centered}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "306540356cd87a5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T16:15:22.350008Z",
     "start_time": "2025-09-03T16:15:22.347008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitted mean: 2.0\n",
      "  Centered data: [-1.0, 0.0, 1.0]\n",
      "  Sum of [1.0, 2.0, 3.0] = 6.0\n",
      "After first run: {'data': [1.0, 2.0, 3.0], 'centered_data': [-1.0, 0.0, 1.0], 'sum': 6.0}\n",
      "Mean learned: 2.0\n",
      "  Centered data: [0.0, 2.0, 4.0]\n",
      "  Sum of [2.0, 4.0, 6.0] = 12.0\n",
      "Second run (no refit): {'data': [2.0, 4.0, 6.0], 'centered_data': [0.0, 2.0, 4.0], 'sum': 12.0}\n",
      "Mean still: 2.0\n",
      "  Fitted mean: 4.0\n",
      "  Centered data: [-2.0, 0.0, 2.0]\n",
      "  Sum of [2.0, 4.0, 6.0] = 12.0\n",
      "With refit=True: {'data': [2.0, 4.0, 6.0], 'centered_data': [-2.0, 0.0, 2.0], 'sum': 12.0}\n",
      "New mean learned: 4.0\n"
     ]
    }
   ],
   "source": [
    "storage = DictStorage({\"data\": [1.0, 2.0, 3.0]})\n",
    "pipeline = FittablePipeline([MeanCenter(), Sum()], name=\"FitPipe\", refit=False, storage=storage)\n",
    "pipeline.run()\n",
    "print(\"After first run:\", storage.as_dict())\n",
    "print(\"Mean learned:\", pipeline._steps[0].mean)\n",
    "# After first run: {'data': [-1.0, 0.0, 1.0], 'sum': 0}\n",
    "# Mean learned: 2.0\n",
    "\n",
    "# Second run without refit (uses same mean=2.0)\n",
    "storage.set({\"data\": [2.0, 4.0, 6.0]})\n",
    "pipeline.run()\n",
    "print(\"Second run (no refit):\", storage.as_dict())\n",
    "print(\"Mean still:\", pipeline._steps[0].mean)\n",
    "# Second run (no refit): {'data': [0.0, 2.0, 4.0], 'sum': 6}\n",
    "# Mean still: 2.0\n",
    "\n",
    "# Pipeline with refit=True (learns new mean=4.0)\n",
    "storage.set({\"data\": [2.0, 4.0, 6.0]})\n",
    "pipeline = FittablePipeline([MeanCenter(), Sum()], name=\"FitPipeRefit\", refit=True, storage=storage)\n",
    "pipeline.run()\n",
    "print(\"With refit=True:\", storage.as_dict())\n",
    "print(\"New mean learned:\", pipeline._steps[0].mean)\n",
    "# With refit=True: {'data': [-2.0, 0.0, 2.0], 'sum': 0}\n",
    "# New mean learned: 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fcb648",
   "metadata": {},
   "source": [
    "## ObservablePipeline\n",
    "\n",
    "Pipeline integration with automatic event emission for monitoring and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "029aace5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ObservablePipeline Demo ===\n",
      "[GLOBAL] -> Demo\n",
      "[PipelineEvent.PIPELINE_START] -> Demo\n",
      "[GLOBAL] -> Demo.Load\n",
      "[PipelineEvent.STEP_START] -> Demo.Load\n",
      "[GLOBAL] -> Demo.Load\n",
      "[PipelineEvent.STEP_END] -> Demo.Load\n",
      "[GLOBAL] -> Demo.Sum\n",
      "[PipelineEvent.STEP_START] -> Demo.Sum\n",
      "  Sum of [1, 2, 3, 4, 5] = 15\n",
      "[GLOBAL] -> Demo.Sum\n",
      "[PipelineEvent.STEP_END] -> Demo.Sum\n",
      "[GLOBAL] -> Demo\n",
      "[PipelineEvent.PIPELINE_END] -> Demo\n",
      "\n",
      "Final STATE: {'data': [1, 2, 3, 4, 5], 'sum': 15}\n"
     ]
    }
   ],
   "source": [
    "# Create and run an ObservablePipeline\n",
    "bus = EventBus()\n",
    "@bus.on(priority=0)\n",
    "def global_logger(event: Event) -> None:\n",
    "    print(f\"[GLOBAL] -> {event.source}\")\n",
    "\n",
    "@bus.on(PipelineEvent.PIPELINE_START)\n",
    "def global_logger(event: Event) -> None:\n",
    "    print(f\"[{event.type}] -> {event.source}\")\n",
    "\n",
    "@bus.on(PipelineEvent.PIPELINE_END)\n",
    "def global_logger(event: Event) -> None:\n",
    "    print(f\"[{event.type}] -> {event.source}\")\n",
    "\n",
    "@bus.on(PipelineEvent.STEP_START)\n",
    "def global_logger(event: Event) -> None:\n",
    "    print(f\"[{event.type}] -> {event.source}\")\n",
    "\n",
    "@bus.on(PipelineEvent.STEP_END)\n",
    "def global_logger(event: Event) -> None:\n",
    "    print(f\"[{event.type}] -> {event.source}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"=== ObservablePipeline Demo ===\")\n",
    "storage = DictStorage()\n",
    "pipeline = ObservablePipeline(steps=[Load(), Sum()], bus=bus, name=\"Demo\", storage=storage)\n",
    "pipeline.run()\n",
    "\n",
    "print(f\"\\nFinal STATE: {storage.as_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875d7e0",
   "metadata": {},
   "source": [
    "## RepeatablePipeline\n",
    "\n",
    "Pipeline integration for repeating steps a fixed number of times eventually interrupted by a condition\n",
    "\n",
    "`clear_storage=True` means storage is cleared at each iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca824d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RepeatablePipeline with clear_storage=True ===\n",
      "  Sum of [1, 2, 3, 4, 5] = 15\n",
      "  Sum of [1, 2, 3, 4, 5] = 15\n",
      "  Sum of [1, 2, 3, 4, 5] = 15\n",
      "\n",
      "Final STATE: {'data': [1, 2, 3, 4, 5], 'sum': 15, 'tot': 15}\n"
     ]
    }
   ],
   "source": [
    "@Step.needs(\"sum\", \"tot\")\n",
    "class Accumulate(Step):\n",
    "    def bindings(self):\n",
    "        return {\"sum\": \"sum\", \"tot\": \"tot\"}\n",
    "\n",
    "    def compute(self, sum: int, tot: int = 0):\n",
    "        tot += sum\n",
    "        return {\"tot\": tot}\n",
    "\n",
    "print(\"\\n=== RepeatablePipeline with clear_storage=True ===\")\n",
    "storage = DictStorage()\n",
    "pipeline = RepeatablePipeline(steps=[Load(), Sum(), Accumulate()], name=\"RepeatDemo\", storage=storage, count=3, clear_storage=True)\n",
    "pipeline.run()\n",
    "print(f\"\\nFinal STATE: {storage.as_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8999c8",
   "metadata": {},
   "source": [
    "`clear_storage=True` means accumulates over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "733df680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RepeatablePipeline with clear_storage=False ===\n",
      "  Sum of [1, 2, 3, 4, 5] = 15\n",
      "  Sum of [1, 2, 3, 4, 5] = 15\n",
      "  Sum of [1, 2, 3, 4, 5] = 15\n",
      "\n",
      "Final STATE: {'data': [1, 2, 3, 4, 5], 'sum': 15, 'tot': 45}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== RepeatablePipeline with clear_storage=False ===\")\n",
    "storage = DictStorage()\n",
    "pipeline = RepeatablePipeline(steps=[Load(), Sum(), Accumulate()], name=\"RepeatDemo\", storage=storage, count=3, clear_storage=False)\n",
    "pipeline.run()\n",
    "print(f\"\\nFinal STATE: {storage.as_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2fb308",
   "metadata": {},
   "source": [
    "A `StoragePredicate` can be used to stop the pipeline based on storage content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b027c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RepeatablePipeline with storage based stopping condition ===\n",
      "  Sum of [1, 2, 3, 4, 5] = 15\n",
      "  Sum of [1, 2, 3, 4, 5] = 15\n",
      "\n",
      "Final STATE: {'tot': 30, 'data': [1, 2, 3, 4, 5], 'sum': 15}\n"
     ]
    }
   ],
   "source": [
    "def greater_than(key: str, value: int) -> StoragePredicate:\n",
    "    return lambda storage: storage.get([key])[key] > value\n",
    "\n",
    "print(\"\\n=== RepeatablePipeline with storage based stopping condition ===\")\n",
    "storage = DictStorage({\"tot\": 0})\n",
    "pipeline = RepeatablePipeline(steps=[Load(), Sum(), Accumulate()], name=\"RepeatDemo\", storage=storage, count=3, clear_storage=False, predicate=greater_than(\"tot\", 20))\n",
    "pipeline.run()\n",
    "print(f\"\\nFinal STATE: {storage.as_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f9844",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Pipeline Architecture**: Orchestrate sequences of steps with automatic data flow management\n",
    "2. **Step Composition**: Define reusable components with clear input/output contracts\n",
    "3. **Event Hooks**: Respond to pipeline lifecycle events for monitoring and custom behavior\n",
    "4. **Priority Systems**: Control execution order with multi-priority event handlers\n",
    "5. **Fittable Pipelines**: ML-specific workflows with separate fit/transform phases\n",
    "6. **Observable Pipelines**: Automatic event emission for external monitoring\n",
    "7. **Repeatable Pipelines**: Conditional re-execution based on storage state\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
