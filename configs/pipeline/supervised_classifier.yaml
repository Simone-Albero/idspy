preprocessing:
  # Non-fitted steps
  base_steps:
    - _target_: load_data
      file_path: ${path.data_raw}
      file_name: ${data.file_name}
      fmt: "csv"
      numerical_cols: ${data.numerical_columns}
      categorical_cols: ${data.categorical_columns}
      target_col: ${data.target_column}

    - _target_: drop_nulls

    - _target_: rare_class_filter
      target_col: ${data.target_column}
      min_count: 3000

    - _target_: stratified_split
      class_column: ${data.target_column}
      train_size: ${data.train_size}
      val_size: ${data.val_size}
      test_size: ${data.test_size}

  # Fitted steps (require fit/transform)
  fitted_steps:
    - _target_: standard_scale

    - _target_: frequency_map
      max_levels: ${max_cat_levels}

    - _target_: label_map


  final_steps:
    - _target_: save_data
      file_path: ${path.data_processed}
      file_name: ${data.file_name}
      fmt: ${data.format}

training:
  # Setup training environment
  setup_steps:
    - _target_: load_data
      file_path: ${path.data_processed}
      file_name: ${data.file_name}
      fmt: ${data.format}

    - _target_: allocate_split_partitions

    - _target_: build_model
      model_config: ${model}
      model_key: model

    - _target_: build_loss
      loss_config: ${loss}
      loss_key: loss_fn

    - _target_: build_optimizer
      optimizer_config: ${optimizer}
      model_key: model
      optimizer_key: optimizer

    - _target_: build_dataset
      df_key: train.data
      dataset_key: train.dataset

    - _target_: build_data_loader
      dataset_key: train.dataset
      dataloader_key: train.dataloader
      batch_size: ${loop.train.dataloader.batch_size}
      num_workers: ${loop.train.dataloader.num_workers}
      shuffle: ${loop.train.dataloader.shuffle}
      pin_memory: ${loop.train.dataloader.pin_memory}

    - _target_: build_scheduler
      scheduler_config: ${scheduler}
      optimizer_key: optimizer
      scheduler_key: scheduler
      dataloader_key: train.dataloader

    - _target_: build_dataset
      df_key: val.data
      dataset_key: val.dataset

    - _target_: build_data_loader
      dataset_key: val.dataset
      dataloader_key: val.dataloader
      batch_size: ${loop.val.dataloader.batch_size}
      num_workers: ${loop.val.dataloader.num_workers}
      shuffle: ${loop.val.dataloader.shuffle}
      pin_memory: ${loop.val.dataloader.pin_memory}

  # Main training loop
  base_steps:
    # Training step
    - _target_: train_one_epoch
      metrics_key: train.metrics

    - _target_: metrics_logger
      log_dir: ${path.logs}
      metrics_key: train.metrics

    - _target_: weights_logger
      log_dir: ${path.logs}
      model_key: model

    # Validation step
    - _target_: validate_one_epoch
      dataloader_key: val.dataloader
      metrics_key: val.metrics
      outputs_key: val.outputs
      save_outputs: false

    - _target_: early_stopping
      min_delta: 0.001
      metrics_key: val.metrics
      model_key: model
      stop_key: stop_pipeline

  final_steps:
    - _target_: save_model_weights
      file_path: ${path.models}
      file_name: ${model._target_}_final
      fmt: pt


testing:
  # Setup testing environment
  setup_steps:
    - _target_: load_data
      file_path: ${path.data_processed}
      file_name: ${data.file_name}
      fmt: ${data.format}

    - _target_: allocate_split_partitions

    - _target_: allocate_targets
      df_key: test.data
      targets_key: test.targets

    - _target_: build_model
      model_config: ${model}
      model_key: model

    - _target_: load_model_weights
      file_path: ${path.models}
      file_name: ${model._target_}_final
      fmt: pt
      model_key: model

    - _target_: build_pred_fn
      pred_config: ${pred_fn}
      pred_key: pred_fn

    - _target_: build_dataset
      df_key: test.data
      dataset_key: test.dataset

    - _target_: build_data_loader
      dataset_key: test.dataset
      dataloader_key: test.dataloader
      batch_size: ${loop.test.dataloader.batch_size}
      num_workers: ${loop.test.dataloader.num_workers}
      shuffle: ${loop.test.dataloader.shuffle}
      pin_memory: ${loop.test.dataloader.pin_memory}

  # Main testing loop
  base_steps:
    - _target_: validate_one_epoch
      dataloader_key: test.dataloader
      metrics_key: test.metrics
      outputs_key: test.outputs
      save_outputs: true

    - _target_: cat_tensors
      inputs_key: test.outputs
      input_section: logits
      outputs_key: test.logits

    - _target_: cat_tensors
      inputs_key: test.outputs
      input_section: latents
      outputs_key: test.latents

    - _target_: make_predictions
      inputs_key: test.logits
      outputs_key: test.predictions

    - _target_: classification_metrics
      predictions_key: test.predictions
      targets_key: test.targets
      metrics_key: test.metrics

    - _target_: clustering_metrics
      vectors_key: test.latents
      targets_key: test.targets
      outputs_key: test.clustering_scores

    - _target_: vectors_projection_plot
      vectors_key: test.latents
      targets_key: test.targets
      n_components: 2
      output_key: test.projection_plots

    - _target_: metrics_logger
      log_dir: ${path.logs}
      metrics_key: test.metrics

    - _target_: metrics_logger
      log_dir: ${path.logs}
      metrics_key: test.clustering_scores

    - _target_: metrics_logger
      log_dir: ${path.logs}
      metrics_key: test.projection_plots
